{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to base (Python 3.11.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts (train): 0.9606124357176251\n",
      "raw_counts (test): 0.9601803155522164\n",
      "tfidf (train): 0.9607126160422093\n",
      "tfidf (test): 0.9619834710743802\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Un-comment this to fix the random seed\n",
    "random.seed(31)\n",
    "\n",
    "\n",
    "def read_file_to_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.strip().split(\",\") for line in file if line.strip()]\n",
    "\n",
    "\n",
    "music = read_file_to_sentences(\"category10.txt\")\n",
    "sports = read_file_to_sentences(\"category17.txt\")\n",
    "\n",
    "vocabulary = sorted(set(token for sentence in music + sports for token in sentence)) + [\n",
    "    None\n",
    "]\n",
    "\n",
    "vocabulary_map = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def onehot(\n",
    "    vocabulary_map: Mapping[Optional[str], int], token: Optional[str]\n",
    ") -> FloatArray:\n",
    "    \"\"\"Generate the one-hot encoding for the provided token in the provided vocabulary.\"\"\"\n",
    "    embedding = np.zeros((len(vocabulary_map),))\n",
    "    idx = vocabulary_map.get(token, len(vocabulary_map) - 1)\n",
    "    embedding[idx] = 1\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    \"\"\"Sum the token embeddings.\"\"\"\n",
    "    total: FloatArray = np.array(token_embeddings).sum(axis=0)\n",
    "    return total\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 10\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    X_train = X[training_idx, :]\n",
    "    y_train = y[training_idx]\n",
    "    X_test = X[testing_idx, :]\n",
    "    y_test = y[testing_idx]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_token_counts(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with raw token counts for the two categories.\"\"\"\n",
    "\n",
    "    # Aggregate embeddings for each category\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in music_document\n",
    "        ]\n",
    "        + [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in sports_document\n",
    "        ]\n",
    "    )\n",
    "    # Generate labels for each category\n",
    "    # Assuming music:0, sports:1\n",
    "    y: FloatArray = np.array(\n",
    "        [0 for sentence in music_document] + [1 for sentence in sports_document]\n",
    "    )\n",
    "\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def generate_data_tfidf(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with TF-IDF scaling.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music_document, sports_document\n",
    "    )\n",
    "    tfidf = TfidfTransformer(norm=None).fit(X_train)\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_lsa(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with LSA.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music_document, sports_document\n",
    "    )\n",
    "    lsa = TruncatedSVD(n_components=300).fit(X_train)\n",
    "    X_train = lsa.transform(X_train)\n",
    "    X_test = lsa.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_word2vec(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with word2vec.\"\"\"\n",
    "    # Load pretrained word2vec model from gensim\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    def get_document_vector(sentence: list[str]) -> NDArray:\n",
    "        \"\"\"Return document vector by summing word vectors.\"\"\"\n",
    "        vectors = [model[word] for word in sentence if word in model.key_to_index]\n",
    "        if vectors:\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(\n",
    "                300\n",
    "            )  # return zero vector if no word in the document has a pretrained vector\n",
    "\n",
    "    # Produce document vectors for each sentence\n",
    "    X = np.array(\n",
    "        [get_document_vector(sentence) for sentence in music_document + sports_document]\n",
    "    )\n",
    "    y = np.array(\n",
    "        [0 for sentence in music_document] + [1 for sentence in sports_document]\n",
    "    )\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def run_experiment() -> None:\n",
    "    \"\"\"Compare performance with different embeddings using Naive Bayes.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(music, sports)\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "    print(\"raw counts (train):\", clf.score(X_train, y_train))\n",
    "    print(\"raw_counts (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "    X_train, y_train, X_test, y_test = generate_data_tfidf(music, sports)\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "    print(\"tfidf (train):\", clf.score(X_train, y_train))\n",
    "    print(\"tfidf (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
