{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category ID 17: \"Sports\"\n",
    "\n",
    "Category ID 10: \"Music\"\n",
    "\n",
    "Category ID 20: \"Gaming\"\n",
    "\n",
    "Category ID 24: \"Entertainment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two aggregration approach - \"sentence-by-sentence\" VS \"whole document\"\n",
    "### 1. Treating Each Video's Tags as a \"Sentence\"\n",
    "\n",
    "In this approach, you consider the tags of each video as a single \"sentence\". This method allows you to maintain the context of which tags belong to which video, which can be valuable for certain types of analysis.\n",
    "\n",
    "**Pros:**\n",
    "- **Context Preservation:** Keeping tags grouped by video preserves the context, which is crucial if the relationships between tags within a video are important.\n",
    "- **Individual Video Analysis:** It allows for analysis at the video level, such as understanding common tag combinations within individual videos.\n",
    "- **Granular Data:** More granular data can lead to more detailed insights, especially if the goal is to understand tagging patterns or to generate video-specific recommendations or classifications.\n",
    "\n",
    "**Cons:**\n",
    "- **Complexity:** More complex to implement, as you need to maintain the association between tags and their respective videos.\n",
    "- **Data Sparsity:** If tags are not repeated often across videos, it might result in sparse data, which can be challenging for some machine learning models.\n",
    "\n",
    "### 2. Aggregating All Tags from a Category into One Document\n",
    "\n",
    "In this method, you aggregate all tags from videos in a particular category into a single document, without distinguishing which tags came from which video.\n",
    "\n",
    "**Pros:**\n",
    "- **Simplicity:** Simpler to implement and process since you're dealing with a bulk collection of tags without worrying about their original context.\n",
    "- **Category-Level Analysis:** Useful for understanding broad trends and commonalities across an entire category of videos.\n",
    "- **Better for Large-Scale Patterns:** Can be more effective for identifying overarching patterns that define a category.\n",
    "\n",
    "**Cons:**\n",
    "- **Loss of Context:** Loses the specific context of which tags are used together in individual videos.\n",
    "- **Potential Noise:** If certain tags are overly common or generic across many videos, they might dominate the dataset and potentially skew the analysis.\n",
    "\n",
    "### Deciding on the Approach\n",
    "\n",
    "- **Goal-Oriented:** Consider what you want to achieve with the analysis. If you are interested in patterns within individual videos or in understanding how certain tags cluster within videos, the first approach is better. If your goal is to understand broader trends across a category, the second approach might be more suitable.\n",
    "- **Data Size and Quality:** The amount and quality of data you have can also influence your choice. If you have a large number of videos with a diverse range of tags, the second approach might provide more meaningful insights at a category level.\n",
    "- **Machine Learning Considerations:** If you're using machine learning, think about what kind of features and labels will be most effective for your model. The granularity of your data can significantly affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnormalized Sentence by sentence aggregation with 4 embeddings (Plain, TF-IDF, LSA, Word to Vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare token/document vectors for classification.\"\"\"\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "# print(api.info())  # show info about available models/datasets\n",
    "\n",
    "# Un-comment this to fix the random seed\n",
    "random.seed(31)\n",
    "\n",
    "\n",
    "def read_file_to_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.strip().split(\",\") for line in file if line.strip()]\n",
    "\n",
    "\n",
    "music = read_file_to_sentences(\"category10.txt\")\n",
    "print(music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Compare token/document vectors for classification.\"\"\"\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "# print(api.info())  # show info about available models/datasets\n",
    "\n",
    "# Un-comment this to fix the random seed\n",
    "random.seed(31)\n",
    "\n",
    "\n",
    "def read_file_to_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.strip().split(\",\") for line in file if line.strip()]\n",
    "\n",
    "\n",
    "music = read_file_to_sentences(\"category10.txt\")\n",
    "sports = read_file_to_sentences(\"category17.txt\")\n",
    "gaming = read_file_to_sentences(\"category20.txt\")\n",
    "entertainment = read_file_to_sentences(\"category24.txt\")\n",
    "\n",
    "vocabulary = sorted(\n",
    "    set(\n",
    "        token\n",
    "        for sentence in music + sports + gaming + entertainment\n",
    "        for token in sentence\n",
    "    )\n",
    ") + [None]\n",
    "\n",
    "vocabulary_map = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def onehot(\n",
    "    vocabulary_map: Mapping[Optional[str], int], token: Optional[str]\n",
    ") -> FloatArray:\n",
    "    \"\"\"Generate the one-hot encoding for the provided token in the provided vocabulary.\"\"\"\n",
    "    embedding = np.zeros((len(vocabulary_map),))\n",
    "    idx = vocabulary_map.get(token, len(vocabulary_map) - 1)\n",
    "    embedding[idx] = 1\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    \"\"\"Sum the token embeddings.\"\"\"\n",
    "    total: FloatArray = np.array(token_embeddings).sum(axis=0)\n",
    "    return total\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 10\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    X_train = X[training_idx, :]\n",
    "    y_train = y[training_idx]\n",
    "    X_test = X[testing_idx, :]\n",
    "    y_test = y[testing_idx]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_token_counts(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    "    gaming_document: list[list[str]],\n",
    "    entertainment_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with raw token counts for four categories.\"\"\"\n",
    "\n",
    "    # Aggregate embeddings for each category\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in music_document\n",
    "        ]\n",
    "        + [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in sports_document\n",
    "        ]\n",
    "        + [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in gaming_document\n",
    "        ]\n",
    "        + [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in entertainment_document\n",
    "        ]\n",
    "    )\n",
    "    # Generate labels for each category\n",
    "    # Assuming music:0, sports:1, gaming:2, entertainment:3\n",
    "    y: FloatArray = np.array(\n",
    "        [0 for sentence in music_document]\n",
    "        + [1 for sentence in sports_document]\n",
    "        + [2 for sentence in gaming_document]\n",
    "        + [3 for sentence in entertainment_document]\n",
    "    )\n",
    "\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def generate_data_tfidf(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    "    gaming_document: list[list[str]],\n",
    "    entertainment_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with TF-IDF scaling.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music_document, sports_document, gaming_document, entertainment_document\n",
    "    )\n",
    "    tfidf = TfidfTransformer(norm=None).fit(X_train)\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_lsa(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    "    gaming_document: list[list[str]],\n",
    "    entertainment_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with LSA.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music_document, sports_document, gaming_document, entertainment_document\n",
    "    )\n",
    "    lsa = TruncatedSVD(n_components=300).fit(X_train)\n",
    "    X_train = lsa.transform(X_train)\n",
    "    X_test = lsa.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_word2vec(\n",
    "    music_document: list[list[str]],\n",
    "    sports_document: list[list[str]],\n",
    "    gaming_document: list[list[str]],\n",
    "    entertainment_document: list[list[str]],\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with word2vec.\"\"\"\n",
    "    # Load pretrained word2vec model from gensim\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    def get_document_vector(sentence: list[str]) -> NDArray:\n",
    "        \"\"\"Return document vector by summing word vectors.\"\"\"\n",
    "        vectors = [model[word] for word in sentence if word in model.key_to_index]\n",
    "        if vectors:\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(\n",
    "                300\n",
    "            )  # return zero vector if no word in the document has a pretrained vector\n",
    "\n",
    "    # Produce document vectors for each sentence\n",
    "    X = np.array(\n",
    "        [\n",
    "            get_document_vector(sentence)\n",
    "            for sentence in music_document\n",
    "            + sports_document\n",
    "            + gaming_document\n",
    "            + entertainment_document\n",
    "        ]\n",
    "    )\n",
    "    y = np.array(\n",
    "        [0 for sentence in music_document]\n",
    "        + [1 for sentence in sports_document]\n",
    "        + [2 for sentence in gaming_document]\n",
    "        + [3 for sentence in entertainment_document]\n",
    "    )\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def run_experiment() -> None:\n",
    "    \"\"\"Compare performance with different embeddiings.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music, sports, gaming, entertainment\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"raw counts (train):\", clf.score(X_train, y_train))\n",
    "    print(\"raw_counts (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_tfidf(\n",
    "        music, sports, gaming, entertainment\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"tfidf (train):\", clf.score(X_train, y_train))\n",
    "    print(\"tfidf (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_lsa(\n",
    "        music, sports, gaming, entertainment\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"lsa (train):\", clf.score(X_train, y_train))\n",
    "    print(\"lsa (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(\n",
    "        music, sports, gaming, entertainment\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"word2vec (train):\", clf.score(X_train, y_train))\n",
    "    print(\"word2vec (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
