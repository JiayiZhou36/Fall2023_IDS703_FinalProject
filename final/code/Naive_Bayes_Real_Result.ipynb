{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to base (Python 3.11.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts (train): 0.9606124357176251\n",
      "raw_counts (test): 0.9601803155522164\n",
      "Time for raw counts section: 92.48 seconds\n",
      "tfidf (train): 0.9608795832498497\n",
      "tfidf (test): 0.9604808414725771\n",
      "Time for tfidf section: 41.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence, Tuple, Dict\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "\n",
    "\n",
    "def read_file_to_sentences(file_path: str) -> List[List[str]]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [line.strip().split(\",\") for line in file if line.strip()]\n",
    "\n",
    "\n",
    "def create_vocabulary_and_mapping(\n",
    "    music: List[List[str]], sports: List[List[str]]\n",
    ") -> Tuple[List[Optional[str]], Dict[Optional[str], int]]:\n",
    "    vocabulary = sorted(\n",
    "        set(token for sentence in music + sports for token in sentence)\n",
    "    ) + [None]\n",
    "    vocabulary_map = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return vocabulary, vocabulary_map\n",
    "\n",
    "\n",
    "def onehot(\n",
    "    vocabulary_map: Mapping[Optional[str], int], token: Optional[str]\n",
    ") -> FloatArray:\n",
    "    embedding = np.zeros((len(vocabulary_map),))\n",
    "    idx = vocabulary_map.get(token, len(vocabulary_map) - 1)\n",
    "    embedding[idx] = 1\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    return np.array(token_embeddings).sum(axis=0)\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 10\n",
    ") -> Tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    return X[training_idx, :], y[training_idx], X[testing_idx, :], y[testing_idx]\n",
    "\n",
    "\n",
    "def generate_data_token_counts(\n",
    "    music_document: List[List[str]],\n",
    "    sports_document: List[List[str]],\n",
    "    vocabulary_map: Mapping[Optional[str], int],\n",
    ") -> Tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in music_document + sports_document\n",
    "        ]\n",
    "    )\n",
    "    y: FloatArray = np.array([0] * len(music_document) + [1] * len(sports_document))\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def generate_data_tfidf(\n",
    "    X_train: FloatArray, X_test: FloatArray\n",
    ") -> Tuple[FloatArray, FloatArray]:\n",
    "    tfidf = TfidfTransformer(norm=None).fit(X_train)\n",
    "    return tfidf.transform(X_train), tfidf.transform(X_test)\n",
    "\n",
    "\n",
    "def run_experiment() -> None:\n",
    "    random.seed(31)\n",
    "    music = read_file_to_sentences(\"category10.txt\")\n",
    "    sports = read_file_to_sentences(\"category17.txt\")\n",
    "    vocabulary, vocabulary_map = create_vocabulary_and_mapping(music, sports)\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        music, sports, vocabulary_map\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "    print(\"raw counts (train):\", clf.score(X_train, y_train))\n",
    "    print(\"raw_counts (test):\", clf.score(X_test, y_test))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time for raw counts section: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_train_tfidf, X_test_tfidf = generate_data_tfidf(X_train, X_test)\n",
    "    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "    print(\"tfidf (train):\", clf.score(X_train_tfidf, y_train))\n",
    "    print(\"tfidf (test):\", clf.score(X_test_tfidf, y_test))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time for tfidf section: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
